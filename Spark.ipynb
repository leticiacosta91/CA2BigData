{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c2e58c4",
   "metadata": {},
   "source": [
    "# PySpark - Project Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97c6982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f35378",
   "metadata": {},
   "source": [
    "#  Initialize Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4055944b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 19:58:57 WARN Utils: Your hostname, muhammad-Vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/05/07 19:58:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/07 19:59:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('project_tweets').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395815d",
   "metadata": {},
   "source": [
    "# Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b828ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the ProjectTweets into Hadoop in the named folder 'CA2BD'\n",
    "\n",
    "data = spark.read.csv('hdfs://localhost:9000/user/hduser/CA2BD/New_Tweets.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e58834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b9dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1600000\n",
      "Number of columns: 5\n"
     ]
    }
   ],
   "source": [
    "num_rows = data.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "\n",
    "num_columns = len(data.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb433f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/07 20:00:08 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "|summary|                 ids|                date|    flag|               user|                text|\n",
      "+-------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "|  count|             1600000|             1600000| 1600000|            1600000|             1600000|\n",
      "|   mean|1.9988175522956276E9|                NULL|    NULL|4.325887521835714E9|                NULL|\n",
      "| stddev| 1.935760736226745E8|                NULL|    NULL|5.16273321845489E10|                NULL|\n",
      "|    min|          1467810369|Fri Apr 17 20:30:...|NO_QUERY|       000catnap000|                 ...|\n",
      "|    25%|          1956912543|                NULL|    NULL|            32508.0|                NULL|\n",
      "|    50%|          2002093413|                NULL|    NULL|           130587.0|                NULL|\n",
      "|    75%|          2177048867|                NULL|    NULL|          1100101.0|                NULL|\n",
      "|    max|          2329205794|Wed May 27 07:27:...|NO_QUERY|         zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c35ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark = spark.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2c34a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.withColumn(\"date\", to_timestamp(data[\"date\"], \"EEE MMM dd HH:mm:ss zzz yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046424e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.select(\"ids\", \"date\", \"user\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11c14c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+--------------------+\n",
      "|       ids|               date|           user|                text|\n",
      "+----------+-------------------+---------------+--------------------+\n",
      "|1467810369|2009-04-07 06:19:45|_TheSpecialOne_|@switchfoot http:...|\n",
      "|1467810672|2009-04-07 06:19:49|  scotthamilton|is upset that he ...|\n",
      "|1467810917|2009-04-07 06:19:53|       mattycus|@Kenichan I dived...|\n",
      "|1467811184|2009-04-07 06:19:57|        ElleCTF|my whole body fee...|\n",
      "|1467811193|2009-04-07 06:19:57|         Karoli|@nationwideclass ...|\n",
      "+----------+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebfd9534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1e89289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, count, format_string, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecaaae72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------+----------+\n",
      "|year|month|count |percentage|\n",
      "+----+-----+------+----------+\n",
      "|2009|4    |100025|6.25%     |\n",
      "|2009|5    |554060|34.63%    |\n",
      "|2009|6    |945915|59.12%    |\n",
      "+----+-----+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = tweets.groupBy(year(\"date\").alias(\"year\"), month(\"date\").alias(\"month\")).count() \\\n",
    "                 .orderBy([\"year\", \"month\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"percentage\", format_string(\"%.2f%%\", ((col(\"count\")/tweets.count())*100)))\n",
    "\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced179bc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083efb6",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1110067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a2263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "tweets_csv = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CA2BD/tweets.csv\", inferSchema=True, header=True)\n",
    "tweets_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f8aa3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select only \"SentimentText\" and \"Sentiment\" column, \n",
    "#and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data.show(truncate = False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "073541a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1350 ; Testing data rows: 582\n"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "256681ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6aedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------------------------------+\n",
      "|SentimentText            |label|SentimentWords                |\n",
      "+-------------------------+-----+------------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |\n",
      "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|\n",
      "+-------------------------+-----+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "125a143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|SentimentText            |label|SentimentWords                |MeaningfulWords            |\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |[adore, cheese, #bestever] |\n",
      "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|[adore, cheese, #brilliant]|\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |[adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |[adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|[adore, cheese, #thumbs-up]|\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d2da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|SentimentText            |label|SentimentWords                |MeaningfulWords            |\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "|I adore cheese #bestever |1    |[i, adore, cheese, #bestever] |[adore, cheese, #bestever] |\n",
      "|I adore cheese #brilliant|1    |[i, adore, cheese, #brilliant]|[adore, cheese, #brilliant]|\n",
      "|I adore cheese #favorite |1    |[i, adore, cheese, #favorite] |[adore, cheese, #favorite] |\n",
      "|I adore cheese #loveit   |1    |[i, adore, cheese, #loveit]   |[adore, cheese, #loveit]   |\n",
      "|I adore cheese #thumbs-up|1    |[i, adore, cheese, #thumbs-up]|[adore, cheese, #thumbs-up]|\n",
      "+-------------------------+-----+------------------------------+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4fc34fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+-------------------------------------------+\n",
      "|label|MeaningfulWords            |features                                   |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #favorite] |(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d8563",
   "metadata": {},
   "source": [
    "# Train our classifier model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e67f04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b9c1f",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e50609d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|Label|MeaningfulWords                      |features                                               |\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "|1    |[adore, classical, music, #bestever] |(262144,[91011,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
      "|1    |[adore, classical, music, #brilliant]|(262144,[45361,100089,102383,131250],[1.0,1.0,1.0,1.0])|\n",
      "+-----+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a523e5",
   "metadata": {},
   "source": [
    "# Predicting testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fe4687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2728cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+----------+-----+\n",
      "|MeaningfulWords                      |prediction|Label|\n",
      "+-------------------------------------+----------+-----+\n",
      "|[adore, classical, music, #bestever] |1.0       |1    |\n",
      "|[adore, classical, music, #brilliant]|1.0       |1    |\n",
      "|[adore, classical, music, #favorite] |1.0       |1    |\n",
      "|[adore, classical, music, #loveit]   |1.0       |1    |\n",
      "+-------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct prediction: 572 , total data: 582 , accuracy: 0.9828178694158075\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=4, truncate = False)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3128407",
   "metadata": {},
   "source": [
    "# Tweets Dataset - cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eae565a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08674b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de expressões regulares\n",
    "at_regex = r\"@\\w+\"  # Remove usernames\n",
    "link_regex = r\"http\\S+\"  # Remove links\n",
    "rt_regex = r'\\bRT\\b'  # Remove 'RT'\n",
    "ss_regex = r'[^\\w\\s]'  # Remove Special strings\n",
    "ds_regex = r'\\s+'  # Remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c58c7aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
      "|ids       |date               |user           |text                                                                                                               |clean_tweet                                                                                             |\n",
      "+----------+-------------------+---------------+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
      "|1467810369|2009-04-07 06:19:45|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D| Awww thats a bummer You shoulda got David Carr of Third Day to do it D                                 |\n",
      "|1467810672|2009-04-07 06:19:49|scotthamilton  |is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he cant update his Facebook by texting it and might cry as a result School today also Blah|\n",
      "|1467810917|2009-04-07 06:19:53|mattycus       |@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | I dived many times for the ball Managed to save 50 The rest go out of bounds                           |\n",
      "|1467811184|2009-04-07 06:19:57|ElleCTF        |my whole body feels itchy and like its on fire                                                                     |my whole body feels itchy and like its on fire                                                          |\n",
      "|1467811193|2009-04-07 06:19:57|Karoli         |@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.     | no its not behaving at all im mad why am i here because I cant see you all over there                  |\n",
      "+----------+-------------------+---------------+-------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"text\", at_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", link_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", rt_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", ss_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", ds_regex, \" \"))\n",
    "\n",
    "# Exibição dos resultados\n",
    "tweets.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3860c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e43100b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     MeaningfulWords|            features|\n",
      "+--------------------+--------------------+\n",
      "|[, awww, thats, b...|(262144,[10345,52...|\n",
      "|[upset, cant, upd...|(262144,[59577,61...|\n",
      "|[, dived, many, t...|(262144,[2548,392...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"clean_tweet\", outputCol=\"words\")\n",
    "tokenizedData = tokenizer.transform(tweets)\n",
    "\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemoved = swr.transform(tokenizedData)\n",
    "\n",
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericData = hashTF.transform(SwRemoved).select('MeaningfulWords', 'features')\n",
    "\n",
    "\n",
    "numericData.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb152238",
   "metadata": {},
   "source": [
    "# Predictiong Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34c99f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIE\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb4d3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(numericData)\n",
    "\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e0ab87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------+----------+\n",
      "|MeaningfulWords                                                                        |prediction|\n",
      "+---------------------------------------------------------------------------------------+----------+\n",
      "|[, awww, thats, bummer, shoulda, got, david, carr, third, day, d]                      |0.0       |\n",
      "|[upset, cant, update, facebook, texting, might, cry, result, school, today, also, blah]|0.0       |\n",
      "|[, dived, many, times, ball, managed, save, 50, rest, go, bounds]                      |0.0       |\n",
      "|[whole, body, feels, itchy, like, fire]                                                |1.0       |\n",
      "|[, behaving, im, mad, cant, see]                                                       |0.0       |\n",
      "|[, whole, crew]                                                                        |0.0       |\n",
      "|[need, hug]                                                                            |0.0       |\n",
      "|[, hey, long, time, see, yes, rains, bit, bit, lol, im, fine, thanks, hows]            |0.0       |\n",
      "|[, nope, didnt]                                                                        |0.0       |\n",
      "|[, que, muera]                                                                         |0.0       |\n",
      "+---------------------------------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 51:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictionFinal.show(truncate = False, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54a1947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionFinal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "758519c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "|               date|           user|                text|         clean_tweet|prediction|\n",
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "|2009-04-07 06:19:45|_TheSpecialOne_|@switchfoot http:...| Awww thats a bum...|       0.0|\n",
      "|2009-04-07 06:20:03|        mybirch|         Need a hug |         Need a hug |       0.0|\n",
      "|2009-04-07 06:20:03|           coZZ|@LOLTrish hey  lo...| hey long time no...|       0.0|\n",
      "|2009-04-07 06:20:09|        mimismo|@twittera que me ...|       que me muera |       0.0|\n",
      "|2009-04-07 06:20:25|       armotley|about to file taxes |about to file taxes |       0.0|\n",
      "|2009-04-07 06:20:34|      gi_gi_bee|@FakerPattyPattz ...| Oh dear Were you...|       0.0|\n",
      "|2009-04-07 06:20:40|      cooliodoc|@angry_barista I ...| I baked you a ca...|       0.0|\n",
      "|2009-04-07 06:20:44|  ChicagoCubbie|I hate when I hav...|I hate when I hav...|       0.0|\n",
      "|2009-04-07 06:20:50|    KatieAngell|Just going to cry...|Just going to cry...|       0.0|\n",
      "|2009-04-07 06:20:52|          gagoo|im sad now  Miss....|im sad now MissLilly|       0.0|\n",
      "|2009-04-07 06:20:56|        abel209|ooooh.... LOL  th...|ooooh LOL that le...|       0.0|\n",
      "|2009-04-07 06:21:04|BaptisteTheFool|Meh... Almost Lov...|Meh Almost Lover ...|       0.0|\n",
      "|2009-04-07 06:21:07|          EmCDL|@alielayus I want...| I want to go to ...|       0.0|\n",
      "|2009-04-07 06:21:09|       merisssa|thought sleeping ...|thought sleeping ...|       0.0|\n",
      "|2009-04-07 06:21:11|       Pbearfox|@julieebaby awe i...| awe i love you t...|       1.0|\n",
      "|2009-04-07 06:21:21|           jsoo|@HumpNinja I cry ...| I cry my asian e...|       0.0|\n",
      "|2009-04-07 06:21:39| Anthony_Nguyen|Bed. Class 8-12. ...|Bed Class 812 Wor...|       0.0|\n",
      "|2009-04-07 06:21:46|      lionslamb|He's the reason f...|Hes the reason fo...|       0.0|\n",
      "|2009-04-07 06:21:53| DdubsShellBell|@JonathanRKnight ...| Awww I soo wish ...|       0.0|\n",
      "|2009-04-07 06:22:23|         tautao|Broadband plan 'a...|Broadband plan a ...|       0.0|\n",
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a column with id following the data's order \n",
    "tweets = tweets.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "predictionFinal = predictionFinal.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# join by \"row_id\"\n",
    "tweets_pred = tweets.select('row_id','date','user', 'text', 'clean_tweet') \\\n",
    "                    .join(predictionFinal.select('row_id', 'prediction'), \"row_id\", \"inner\")\n",
    "                \n",
    "\n",
    "# drop column \n",
    "tweets_pred = tweets_pred.drop(\"row_id\")\n",
    "\n",
    "tweets_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691d7b2",
   "metadata": {},
   "source": [
    "# Textblod and Varder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2427f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b679287c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d9dc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Função de análise de sentimento com TextBlob\n",
    "@udf(FloatType())\n",
    "def sentiment(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "# Função de análise de sentimento com VADER\n",
    "@udf(FloatType())\n",
    "def sentiment_vader(tweet):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(tweet)['compound']\n",
    "\n",
    "# Aplicar diretamente no DataFrame\n",
    "tweets_pred = tweets_pred.withColumn(\"textblob\", sentiment(tweets_pred[\"clean_tweet\"])) \\\n",
    "                         .withColumn(\"vader\", sentiment_vader(tweets_pred[\"clean_tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ab1fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pred = tweets_pred.withColumn(\"score\", ((col(\"prediction\") + (col(\"textblob\")*1.5) + (col(\"vader\")*1.5)) / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4991a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+-------+--------------------+\n",
      "|         clean_tweet|prediction|   textblob|  vader|               score|\n",
      "+--------------------+----------+-----------+-------+--------------------+\n",
      "| Awww thats a bum...|       0.0|        0.2|-0.3818|-0.06817499734461308|\n",
      "|is upset that he ...|       0.0|        0.0|-0.7269|-0.27258749306201935|\n",
      "| not the whole crew |       0.0|        0.2|    0.0| 0.07500000111758709|\n",
      "|         Need a hug |       0.0|        0.0| 0.4767|  0.1787625029683113|\n",
      "| hey long time no...|       0.0| 0.27333334| 0.8286|  0.4132249988615513|\n",
      "|       que me muera |       0.0|        0.0|    0.0|                 0.0|\n",
      "|spring break in p...|       0.0|-0.21428572|    0.0|-0.08035714365541935|\n",
      "|about to file taxes |       0.0|        0.0|    0.0|                 0.0|\n",
      "| Oh dear Were you...|       0.0|        0.0| 0.1779|  0.0667125005275011|\n",
      "| I baked you a ca...|       0.0|        0.0|    0.0|                 0.0|\n",
      "+--------------------+----------+-----------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_pred.select(\"clean_tweet\", \"prediction\", \"textblob\", \"vader\", \"score\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e30f93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- clean_tweet: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- textblob: float (nullable = true)\n",
      " |-- vader: float (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43411b9f",
   "metadata": {},
   "source": [
    "# Saving on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e7807f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient\n",
    "from hdfs.util import HdfsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99de0d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2851107323.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_5283/2851107323.py\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    df_salvo = spark_hadoop(tweets_pred, folder=\"sentiment\", partitionBy\"date\", spark=spark)\u001b[0m\n\u001b[0m                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>   (0 + 2) / 4][Stage 70:>   (0 + 0) / 2][Stage 71:>   (0 + 0) / 2]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from hdfs import InsecureClient\n",
    "from hdfs.util import HdfsError\n",
    "\n",
    "def spark_hadoop(df, folder, partitionBy=None, spark=None):\n",
    "    hdfs_base_path = \"hdfs://localhost:9000\"\n",
    "    hdfs_folder_path = f\"{hdfs_base_path}/CA22BD/{folder}\"\n",
    "\n",
    "    client = InsecureClient('http://localhost:9870', user='hduser')\n",
    "\n",
    "    try:\n",
    "       \n",
    "        client.content(hdfs_folder_path)\n",
    "\n",
    "        print('Os arquivos já estão no Hadoop. Lendo os arquivos.')\n",
    "        df = spark.read.parquet(hdfs_folder_path)\n",
    "    except HdfsError:\n",
    "        print('Colocando no Hadoop.')\n",
    "        if partitionBy:\n",
    "            \n",
    "            df = df.withColumn(\"year\", year(\"date\"))\n",
    "            df = df.withColumn(\"month\", month(\"date\"))\n",
    "            df = df.withColumn(\"day\", dayofmonth(\"date\"))\n",
    "            \n",
    "           \n",
    "            df.write.partitionBy(\"year\", \"month\", \"day\").parquet(hdfs_folder_path)\n",
    "            print(f\"Salvo em {hdfs_folder_path} particionado por {partitionBy}\")\n",
    "        else:\n",
    "            df.write.parquet(hdfs_folder_path)\n",
    "            print(f\"Salvo em {hdfs_fol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff057f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_salvo.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f754ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
