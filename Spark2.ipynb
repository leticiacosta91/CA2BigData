{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b859f6",
   "metadata": {},
   "source": [
    "# PySpark - Project Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1858ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark import SparkConf\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, StringType\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3e9b2",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3a35b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 10:33:08 WARN Utils: Your hostname, muhammad-Vm resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/05/10 10:33:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/10 10:33:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('project_tweets').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48119f2",
   "metadata": {},
   "source": [
    "## Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41951db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the ProjectTweets into Hadoop in the named folder 'CA2BD'\n",
    "\n",
    "data = spark.read.csv('hdfs://localhost:9000/user/hduser/CA2BD/New_Tweets.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c43fdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the structure of schema\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835f8dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 100000\n",
      "Number of columns: 5\n"
     ]
    }
   ],
   "source": [
    "num_rows = data.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "\n",
    "num_columns = len(data.columns)\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "897d76ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/10 10:33:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 10:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "|summary|                 ids|                date|    flag|               user|                text|\n",
      "+-------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "|  count|              100000|              100000|  100000|             100000|              100000|\n",
      "|   mean|  1.99847366845205E9|                NULL|    NULL|3.781014978571428E7|                NULL|\n",
      "| stddev|1.9376617856586394E8|                NULL|    NULL|9.776457413992319E7|                NULL|\n",
      "|    min|          1467812025|Fri Apr 17 20:31:...|NO_QUERY|           007peter|             i ju...|\n",
      "|    25%|          1956824746|                NULL|    NULL|            78787.0|                NULL|\n",
      "|    50%|          2002093382|                NULL|    NULL|           132057.0|                NULL|\n",
      "|    75%|          2177004951|                NULL|    NULL|           892300.0|                NULL|\n",
      "|    max|          2329202126|Wed May 27 07:27:...|NO_QUERY|         zzzunzinnn|ï¿½rebro tonight!...|\n",
      "+-------+--------------------+--------------------+--------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea67d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark = spark.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6d92ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.withColumn(\"date\", to_timestamp(data[\"date\"], \"EEE MMM dd HH:mm:ss zzz yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cee2d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.select(\"ids\", \"date\", \"user\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08002847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+--------------------+\n",
      "|       ids|               date|           user|                text|\n",
      "+----------+-------------------+---------------+--------------------+\n",
      "|1976753780|2009-05-31 02:58:12|       kimkins1|umm i kinda wanna...|\n",
      "|1793549629|2009-05-14 11:07:11|       clinamen|testing #talkings...|\n",
      "|1974118439|2009-05-30 20:28:45|      JohanCITI|I now have 2 twit...|\n",
      "|2064596548|2009-06-07 14:41:36|MissDaisyTurner|I HAVE TO SAY IT....|\n",
      "|2055168891|2009-06-06 16:49:43|      wrwarrick|got the feeds wor...|\n",
      "+----------+-------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e21254bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ids: long (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a68b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, count, format_string, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "389aaa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|year|month|count|percentage|\n",
      "+----+-----+-----+----------+\n",
      "|2009|4    |6336 |6.34%     |\n",
      "|2009|5    |34595|34.60%    |\n",
      "|2009|6    |59069|59.07%    |\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = tweets.groupBy(year(\"date\").alias(\"year\"), month(\"date\").alias(\"month\")).count() \\\n",
    "                 .orderBy([\"year\", \"month\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"percentage\", format_string(\"%.2f%%\", ((col(\"count\")/tweets.count())*100)))\n",
    "\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dddfb6d",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b5f12",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22160799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442554a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "tweets_csv = spark.read.csv(\"hdfs://localhost:9000/user/hduser/CA2BD/tweets.csv\", inferSchema=True, header=True)\n",
    "tweets_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf1ca0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select only \"SentimentText\" and \"Sentiment\" column, \n",
    "#and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data.show(truncate = False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb4a777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1379 ; Testing data rows: 553\n"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec12a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b2b821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+---------------------------------------+\n",
      "|SentimentText                    |label|SentimentWords                         |\n",
      "+---------------------------------+-----+---------------------------------------+\n",
      "|I adore cheese #brilliant        |1    |[i, adore, cheese, #brilliant]         |\n",
      "|I adore cheese #favorite         |1    |[i, adore, cheese, #favorite]          |\n",
      "|I adore cheese #loveit           |1    |[i, adore, cheese, #loveit]            |\n",
      "|I adore cheese #thumbs-up        |1    |[i, adore, cheese, #thumbs-up]         |\n",
      "|I adore classical music #bestever|1    |[i, adore, classical, music, #bestever]|\n",
      "+---------------------------------+-----+---------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af038587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+---------------------------------------+------------------------------------+\n",
      "|SentimentText                    |label|SentimentWords                         |MeaningfulWords                     |\n",
      "+---------------------------------+-----+---------------------------------------+------------------------------------+\n",
      "|I adore cheese #brilliant        |1    |[i, adore, cheese, #brilliant]         |[adore, cheese, #brilliant]         |\n",
      "|I adore cheese #favorite         |1    |[i, adore, cheese, #favorite]          |[adore, cheese, #favorite]          |\n",
      "|I adore cheese #loveit           |1    |[i, adore, cheese, #loveit]            |[adore, cheese, #loveit]            |\n",
      "|I adore cheese #thumbs-up        |1    |[i, adore, cheese, #thumbs-up]         |[adore, cheese, #thumbs-up]         |\n",
      "|I adore classical music #bestever|1    |[i, adore, classical, music, #bestever]|[adore, classical, music, #bestever]|\n",
      "+---------------------------------+-----+---------------------------------------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a1c5ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+-------------------------------------------+\n",
      "|label|MeaningfulWords            |features                                   |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #favorite] |(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
      "|1    |[adore, cheese, #loveit]   |(262144,[1689,100089,254974],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4c9fd",
   "metadata": {},
   "source": [
    "## Train our classifier model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3a9184f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ec336",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "507d4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+------------------------------------------+\n",
      "|Label|MeaningfulWords            |features                                  |\n",
      "+-----+---------------------------+------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0])|\n",
      "|1    |[adore, cheese, #toptastic]|(262144,[1689,42010,100089],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e0c28a",
   "metadata": {},
   "source": [
    "## Predicting testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4e62566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1b94578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+----------+-----+\n",
      "|MeaningfulWords                      |prediction|Label|\n",
      "+-------------------------------------+----------+-----+\n",
      "|[adore, cheese, #bestever]           |1.0       |1    |\n",
      "|[adore, cheese, #toptastic]          |1.0       |1    |\n",
      "|[adore, classical, music, #brilliant]|1.0       |1    |\n",
      "|[adore, classical, music, #favorite] |1.0       |1    |\n",
      "+-------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "correct prediction: 542 , total data: 553 , accuracy: 0.9801084990958409\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=4, truncate = False)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0436a253",
   "metadata": {},
   "source": [
    "## Tweets Dataset - cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04673499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9342ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de expressões regulares\n",
    "at_regex = r\"@\\w+\"  # Remove usernames\n",
    "link_regex = r\"http\\S+\"  # Remove links\n",
    "rt_regex = r'\\bRT\\b'  # Remove 'RT'\n",
    "ss_regex = r'[^\\w\\s]'  # Remove Special strings\n",
    "ds_regex = r'\\s+'  # Remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f1a6dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+---------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\n",
      "|ids       |date               |user           |text                                                                                   |clean_tweet                                                                    |\n",
      "+----------+-------------------+---------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\n",
      "|1976753780|2009-05-31 02:58:12|kimkins1       |umm i kinda wanna turn into a dinosaur and eat your face is that okay?? RAWR!!!    hehe|umm i kinda wanna turn into a dinosaur and eat your face is that okay RAWR hehe|\n",
      "|1793549629|2009-05-14 11:07:11|clinamen       |testing #talkingshop (1,2)                                                             |testing talkingshop 12                                                         |\n",
      "|1974118439|2009-05-30 20:28:45|JohanCITI      |I now have 2 twitter accounts I can not use...                                         |I now have 2 twitter accounts I can not use                                    |\n",
      "|2064596548|2009-06-07 14:41:36|MissDaisyTurner|I HAVE TO SAY IT. ZAC EFRON IS MY DREAM BOY! OK I SAID IT                              |I HAVE TO SAY IT ZAC EFRON IS MY DREAM BOY OK I SAID IT                        |\n",
      "|2055168891|2009-06-06 16:49:43|wrwarrick      |got the feeds working on the website.   Time to rejoin the world                       |got the feeds working on the website Time to rejoin the world                  |\n",
      "+----------+-------------------+---------------+---------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"text\", at_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", link_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", rt_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", ss_regex, \"\"))\n",
    "tweets= tweets.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", ds_regex, \" \"))\n",
    "\n",
    "# Exibição dos resultados\n",
    "tweets.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0335017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f09f43f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     MeaningfulWords|            features|\n",
      "+--------------------+--------------------+\n",
      "|[umm, kinda, wann...|(262144,[44094,88...|\n",
      "|[testing, talking...|(262144,[7441,137...|\n",
      "|[2, twitter, acco...|(262144,[1512,125...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"clean_tweet\", outputCol=\"words\")\n",
    "tokenizedData = tokenizer.transform(tweets)\n",
    "\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemoved = swr.transform(tokenizedData)\n",
    "\n",
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericData = hashTF.transform(SwRemoved).select('MeaningfulWords', 'features')\n",
    "\n",
    "\n",
    "numericData.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096ddc2",
   "metadata": {},
   "source": [
    "## Predictiong Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27a9f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIE\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbc0205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(numericData)\n",
    "\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ffd5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------+----------+\n",
      "|MeaningfulWords                                                                          |prediction|\n",
      "+-----------------------------------------------------------------------------------------+----------+\n",
      "|[umm, kinda, wanna, turn, dinosaur, eat, face, okay, rawr, hehe]                         |0.0       |\n",
      "|[testing, talkingshop, 12]                                                               |0.0       |\n",
      "|[2, twitter, accounts, use]                                                              |0.0       |\n",
      "|[say, zac, efron, dream, boy, ok, said]                                                  |0.0       |\n",
      "|[got, feeds, working, website, time, rejoin, world]                                      |0.0       |\n",
      "|[, finale, one, bungees, broke, emergency, silkpink, rope, dropped, didnt, finish, meant]|0.0       |\n",
      "|[summer, vacation, fun]                                                                  |1.0       |\n",
      "|[, hi, jojo, thanks, accept, request, brazilians, kisses]                                |0.0       |\n",
      "|[time, flies, u, clue, ur]                                                               |0.0       |\n",
      "|[, fair, enough, long, live, dorks]                                                      |0.0       |\n",
      "+-----------------------------------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionFinal.show(truncate = False, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8cec8ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionFinal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "210e7f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "|               date|           user|                text|         clean_tweet|prediction|\n",
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "|2009-05-31 02:58:12|       kimkins1|umm i kinda wanna...|umm i kinda wanna...|       0.0|\n",
      "|2009-05-14 11:07:11|       clinamen|testing #talkings...|testing talkingsh...|       0.0|\n",
      "|2009-05-30 20:28:45|      JohanCITI|I now have 2 twit...|I now have 2 twit...|       0.0|\n",
      "|2009-06-07 14:41:36|MissDaisyTurner|I HAVE TO SAY IT....|I HAVE TO SAY IT ...|       0.0|\n",
      "|2009-06-06 16:49:43|      wrwarrick|got the feeds wor...|got the feeds wor...|       0.0|\n",
      "|2009-06-17 13:19:24|       tamjay17|@cazzlar in the f...| in the finale on...|       0.0|\n",
      "|2009-06-01 17:03:30|      mohehnick|Summer vacation i...|Summer vacation i...|       1.0|\n",
      "|2009-06-15 11:46:06|    DanielTeixe|@joannalevesque H...| Hi JoJo Thanks f...|       0.0|\n",
      "|2009-05-30 16:19:10|         jj1223|time flies when u...|time flies when u...|       0.0|\n",
      "|2009-05-02 07:55:46|  stellargellar|@closetoreason fa...| fair enough long...|       0.0|\n",
      "|2009-06-16 06:33:52|    soniajaxson|@yoitsCODY okay  ...| okay night ttyl ...|       0.0|\n",
      "|2009-05-27 05:29:15|          stoja|@brettsea I feel ...| I feel slightly ...|       0.0|\n",
      "|2009-06-01 01:34:24|       jrnygirl|i'm hungry, but d...|im hungry but don...|       1.0|\n",
      "|2009-06-06 00:29:32|         BradyV|@EliteByDesign I ...|       I know right |       0.0|\n",
      "|2009-05-30 22:09:26|          the_3|@gwcaudra Was it ...| Was it better th...|       0.0|\n",
      "|2009-05-18 15:13:32|    iamkatelynn|@juliaundchicco g...| good job youre v...|       1.0|\n",
      "|2009-06-15 22:22:40|     magicconch|@molnica I knowww...| I knowww like no...|       1.0|\n",
      "|2009-05-31 03:38:49|      dkarimian| worst mojito I e...| worst mojito I e...|       0.0|\n",
      "|2009-06-06 15:43:31| armorfordreams|@MaryKateOlsen9 m...| morning i hope y...|       0.0|\n",
      "|2009-05-30 20:46:53|    collierchin|@rainbeauxx hey h...|    hey hun Ummmyea |       0.0|\n",
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a column with id following the data's order \n",
    "tweets = tweets.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "predictionFinal = predictionFinal.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# join by \"row_id\"\n",
    "tweets_pred = tweets.select('row_id','date','user', 'text', 'clean_tweet') \\\n",
    "                    .join(predictionFinal.select('row_id', 'prediction'), \"row_id\", \"inner\")\n",
    "                \n",
    "\n",
    "# drop column \n",
    "tweets_pred = tweets_pred.drop(\"row_id\")\n",
    "\n",
    "tweets_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858706cb",
   "metadata": {},
   "source": [
    "## Textblod and Varder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28be5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d5024a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a2bd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Função de análise de sentimento com TextBlob\n",
    "@udf(FloatType())\n",
    "def sentiment(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "# Função de análise de sentimento com VADER\n",
    "@udf(FloatType())\n",
    "def sentiment_vader(tweet):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(tweet)['compound']\n",
    "\n",
    "# Aplicar diretamente no DataFrame\n",
    "tweets_pred = tweets_pred.withColumn(\"textblob\", sentiment(tweets_pred[\"clean_tweet\"])) \\\n",
    "                         .withColumn(\"vader\", sentiment_vader(tweets_pred[\"clean_tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f0dc02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pred = tweets_pred.withColumn(\"score\", ((col(\"prediction\") + (col(\"textblob\")*1.5) + (col(\"vader\")*1.5)) / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6791aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------+--------------------+\n",
      "|         clean_tweet|prediction|  textblob|  vader|               score|\n",
      "+--------------------+----------+----------+-------+--------------------+\n",
      "|umm i kinda wanna...|       0.0|       0.5| 0.2263| 0.27236250042915344|\n",
      "|testing talkingsh...|       0.0|       0.0|    0.0|                 0.0|\n",
      "|I now have 2 twit...|       0.0|       0.0|    0.0|                 0.0|\n",
      "|I HAVE TO SAY IT ...|       0.0|       0.5| 0.4939| 0.37271250039339066|\n",
      "|got the feeds wor...|       0.0|       0.0|    0.0|                 0.0|\n",
      "| in the finale on...|       0.0|       0.0|-0.6597|-0.24738749116659164|\n",
      "|Summer vacation i...|       1.0|       0.3| 0.5106|  0.5539749935269356|\n",
      "| Hi JoJo Thanks f...|       0.0|       0.2| 0.8316| 0.38685000501573086|\n",
      "|time flies when u...|       0.0|       0.0| -0.296|-0.11100000143051147|\n",
      "| fair enough long...|       0.0|0.19659092| 0.2023|  0.1495840921998024|\n",
      "+--------------------+----------+----------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_pred.select(\"clean_tweet\", \"prediction\", \"textblob\", \"vader\", \"score\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64068f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- clean_tweet: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- textblob: float (nullable = true)\n",
      " |-- vader: float (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970273a1",
   "metadata": {},
   "source": [
    "## Saving on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2a6f736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colocando no Hadoop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo em hdfs://localhost:9000/CA2BD/sentiment particionado por date\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from hdfs import InsecureClient\n",
    "from hdfs.util import HdfsError\n",
    "\n",
    "def spark_hadoop(df, folder, partitionBy=None, spark=None):\n",
    "    hdfs_base_path = \"hdfs://localhost:9000\"\n",
    "    hdfs_folder_path = f\"{hdfs_base_path}/CA2BD/{folder}\"\n",
    "\n",
    "    client = InsecureClient('http://localhost:9870', user='hduser')\n",
    "\n",
    "    try:\n",
    "       \n",
    "        client.content(hdfs_folder_path)\n",
    "\n",
    "        print('Os arquivos já estão no Hadoop. Lendo os arquivos.')\n",
    "        df = spark.read.parquet(hdfs_folder_path)\n",
    "    except HdfsError:\n",
    "        print('Colocando no Hadoop.')\n",
    "        if partitionBy:\n",
    "            \n",
    "            df = df.withColumn(\"year\", year(\"date\"))\n",
    "            df = df.withColumn(\"month\", month(\"date\"))\n",
    "            df = df.withColumn(\"day\", dayofmonth(\"date\"))\n",
    "            \n",
    "           \n",
    "            df.write.partitionBy(\"year\", \"month\", \"day\").parquet(hdfs_folder_path)\n",
    "            print(f\"Salvo em {hdfs_folder_path} particionado por {partitionBy}\")\n",
    "        else:\n",
    "            df.write.parquet(hdfs_folder_path)\n",
    "            print(f\"Salvo em {hdfs_folder_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_salvo = spark_hadoop(tweets_pred, folder=\"sentiment\", partitionBy=\"date\", spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba7b38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+--------------------+--------------------+----------+--------+------+-------------------+----+-----+---+\n",
      "|               date|    user|                text|         clean_tweet|prediction|textblob| vader|              score|year|month|day|\n",
      "+-------------------+--------+--------------------+--------------------+----------+--------+------+-------------------+----+-----+---+\n",
      "|2009-05-31 02:58:12|kimkins1|umm i kinda wanna...|umm i kinda wanna...|       0.0|     0.5|0.2263|0.27236250042915344|2009|    5| 31|\n",
      "|2009-05-14 11:07:11|clinamen|testing #talkings...|testing talkingsh...|       0.0|     0.0|   0.0|                0.0|2009|    5| 14|\n",
      "+-------------------+--------+--------------------+--------------------+----------+--------+------+-------------------+----+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_salvo.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e7503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
